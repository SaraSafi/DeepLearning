{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cd484ca-4837-4aa9-8385-9fc0f10e7132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hazm as hz\n",
    "from hazm import Normalizer, word_tokenize,stopwords_list,Stemmer\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools    \n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,Masking\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6fb533f-9d9e-42ff-b0f5-79455a8bd7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...</td>\n",
       "      <td>SAD</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>عالللی بود همه چه درست و به اندازه و کیفیت خوب...</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>شیرینی وانیلی فقط یک مدل بود.</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                            comment  label  \\\n",
       "0        NaN    واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح    SAD   \n",
       "1        NaN  قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...  HAPPY   \n",
       "2        NaN  قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...    SAD   \n",
       "3        NaN  عالللی بود همه چه درست و به اندازه و کیفیت خوب...  HAPPY   \n",
       "4        NaN                      شیرینی وانیلی فقط یک مدل بود.  HAPPY   \n",
       "\n",
       "   label_id  \n",
       "0       1.0  \n",
       "1       0.0  \n",
       "2       1.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('snappfood.csv', on_bad_lines='skip' , delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7756889a-1eee-4787-bc64-2c53269117de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove nan values\n",
    "df=df[['comment','label_id']]\n",
    "df=df.dropna()\n",
    "normalizer = hz.Normalizer()\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[\\{\\}\\؛\\*\\=\\-\\_\\+\\/\\n]\",\" \",str(text))\n",
    "    text = re.sub(\"[ ]+\",\" \",text)\n",
    "    text = re.sub(\"\\!+\",\"!\",text)\n",
    "    text = re.sub(\"[؟]+\",\"؟\",text)\n",
    "    text = re.sub(\"[.]+\",\"\",text)\n",
    "    text = re.sub(\"[،]+\",\"\",text)\n",
    "    # replace Finglish words with an empty string\n",
    "    finglish_pattern = r\"[a-zA-Z]+\"\n",
    "    if finglish_pattern in text:   \n",
    "        text = re.sub(finglish_pattern, \"\", text)\n",
    "    for c in \"..آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیئ\":\n",
    "        text = re.sub(f\"[{c}]+\", c, text)\n",
    "    # \\u200c:separate two characters that should not be connected,\\r\\n:remove line break\n",
    "    text=text.replace('\\u200c', '').replace('\\r\\n',' ').replace('|',' ')\n",
    "    #normalize the text\n",
    "    text = normalizer.normalize(text)\n",
    "    words = []\n",
    "    words.append(hz.word_tokenize(text))\n",
    "    return words\n",
    "\n",
    "train_data = df['comment'].apply(preprocess)\n",
    "df['comment'] = list(itertools.chain(*train_data))\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords=stopwords_list()\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x if word not in stopwords]))\n",
    "\n",
    "#remove english comments\n",
    "english_text=df[df.comment.str.contains(r'[a-zA-Z]+')]\n",
    "idx=english_text.index\n",
    "df=df.drop(idx).reset_index()\n",
    "\n",
    "# find the stemm of words\n",
    "stemmer = hz.Stemmer()\n",
    "def stem_comment(comment):\n",
    "    return ' '.join([stemmer.stem(word) for word in comment.split()])\n",
    "\n",
    "# Apply stemming to 'comment' column\n",
    "df['comment'] = df['comment'].apply(stem_comment)\n",
    "df=df[['comment','label_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee9e6285-5e84-43bd-9a0c-cd7f8c59f3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'غذا': 1, 'کیف': 2, 'سفار': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 212,  88,  63],\n",
       "       [  0,   0,   0, ...,  41, 202,  42],\n",
       "       [  0,   0,   0, ..., 100,  10,   8],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 837,   4, 246],\n",
       "       [  0,   0,   0, ..., 100,   6,  28],\n",
       "       [  0,   0,   0, ...,  54, 334,   4]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(df['comment'])\n",
    "print(dict(list(tokenizer.word_index.items())[0:3]))\n",
    "#transforms each text in texts to a sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df['comment'])\n",
    "#adding padding to comments\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "# Splitting data into training and testing set\n",
    "y = df['label_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d999a12-6614-4c71-8013-b43010fd92e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "  \n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        count = 0\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "            if not count:\n",
    "                print(\"mask: \", mask)\n",
    "                count += 1\n",
    "        # print(\"input shape: \", inputs.shape)\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "          \"embed_dim\": self.embed_dim,\n",
    "          \"num_heads\": self.num_heads,  \n",
    "          \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf872b2a-3c4d-4b74-a143-256530c7856a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728/1728 [==============================] - 1234s 712ms/step - loss: 0.4506 - accuracy: 0.7905 - val_loss: 0.4374 - val_accuracy: 0.7966\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_17 (Masking)        (None, 100)               0         \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    (None, 100, 256)          5120000   \n",
      "                                                                 \n",
      " transformer_encoder_17 (Tra  (None, 100, 256)         543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "432/432 [==============================] - 81s 186ms/step - loss: 0.4374 - accuracy: 0.7966\n",
      "Test acc: 0.797\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0))\n",
    "    model.add(Embedding(vocab_size, embed_dim))\n",
    "    # Use TransformerEncoder\n",
    "    model.add(TransformerEncoder(embed_dim, dense_dim, num_heads))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model=get_model()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                        save_best_only=True)\n",
    "]\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "          \"transformer_encoder.keras\",\n",
    "          custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(model.summary())\n",
    "print(f\"Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
